---
title: "Machine Learning Project"
author: "S.Somasekar"
date: "September 27, 2015"
output: html_document
---

# Introduction:

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. We will also use the prediction model to predict 20 different test cases. 

# Installing Required Packages:
```{r}

##install.packages("caret")
library(caret)
library(rpart)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(rpart.plot)
```

# Importing Data:
```{r}
 trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  trainFile <- "./data/pml-training.csv"
  testFile  <- "./data/pml-testing.csv"
  if (!file.exists("./data")) {
    dir.create("./data")
  }
  if (!file.exists(trainFile)) {
    download.file(trainUrl, destfile=trainFile, method="curl")
  }
  if (!file.exists(testFile)) {
    download.file(testUrl, destfile=testFile, method="curl")
  }
```

# Reading Data:
```{r}
df_training <- read.csv("pml-training.csv")

df_testing <- read.csv("pml-testing.csv")
```
# Cleaning the data:

N/A values and missing values can significantly affect data processing. Thus, as first step of cleaning the data, we will remove the rows/columns that contain the NA values. To further clean up the data, we will also get rid of some columns that do not contribute much to the accelerometer measurements

```{r}
sum(complete.cases(df_training))

#First, we remove columns that contain NA missing values.
df_training <- df_training[, colSums(is.na(df_training)) == 0] 
df_testing <- df_testing[, colSums(is.na(df_testing)) == 0] 
#Next, we get rid of some columns that do not contribute much to the accelerometer measurements.
classe <- df_training$classe
trainRemove <- grepl("^X|timestamp|window", names(df_training))
df_training <- df_training[, !trainRemove]
trainCleaned <- df_training[, sapply(df_training, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(df_testing))
df_testing <- df_testing[, !testRemove]
testCleaned <- df_testing[, sapply(df_testing, is.numeric)]
```

# Slicing the big dataset into 4 smaller sets:

The original dataset, even after cleaning contains 19622 rows and 53 variables. This is too big a file to handle and the processing will take a long time. Therefore, I decided to split the training dataset to 4 smaller sets. Each set was in turn divided in two parts - 60% for training and 40% for testing. The Modeling methods to be follwed will be applied to all these 4 sub-sets.

```{r}
set.seed(666)
ids_small <- createDataPartition(y=trainCleaned$classe, p=0.25, list=FALSE)
df_small1 <- trainCleaned[ids_small,]
df_remainder <- trainCleaned[-ids_small,]
set.seed(666)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.33, list=FALSE)
df_small2 <- df_remainder[ids_small,]
df_remainder <- df_remainder[-ids_small,]
set.seed(666)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.5, list=FALSE)
df_small3 <- df_remainder[ids_small,]
df_small4 <- df_remainder[-ids_small,]
# Divide each of these 4 sets into training (60%) and test (40%) sets.
set.seed(666)
inTrain <- createDataPartition(y=df_small1$classe, p=0.6, list=FALSE)
df_small_training1 <- df_small1[inTrain,]
df_small_testing1 <- df_small1[-inTrain,]
set.seed(666)
inTrain <- createDataPartition(y=df_small2$classe, p=0.6, list=FALSE)
df_small_training2 <- df_small2[inTrain,]
df_small_testing2 <- df_small2[-inTrain,]
set.seed(666)
inTrain <- createDataPartition(y=df_small3$classe, p=0.6, list=FALSE)
df_small_training3 <- df_small3[inTrain,]
df_small_testing3 <- df_small3[-inTrain,]
set.seed(666)
inTrain <- createDataPartition(y=df_small4$classe, p=0.6, list=FALSE)
df_small_training4 <- df_small4[inTrain,]
df_small_testing4 <- df_small4[-inTrain,]

dim(df_small_training1)
dim(df_small_testing1)

dim(df_small_training2)
dim(df_small_testing2)

dim(df_small_training3)
dim(df_small_testing3)

dim(df_small_training4)
dim(df_small_testing4)
```


# Model Building: 

The model was built using the both pre-processing and Cross Validation using RandomForest as the method to evaluate CV. 

```{r}

# Train on training set 1 of 4 with both preprocessing and cross validation using randomForest method.
set.seed(123)
modFit <- train(df_small_training1$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training1)
print(modFit, digits=3)

# Run against testing set 1 of 4.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)

# Run against 20 testing set.
print(predict(modFit, newdata=testCleaned))


# Train on training set 2 of 4 with both preprocessing and cross validation using randomForest method.
set.seed(123)
modFit1 <- train(df_small_training2$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training2)
print(modFit1, digits=3)

# Run against testing set 2 of 4.
predictions1 <- predict(modFit1, newdata=df_small_testing2)
print(confusionMatrix(predictions1, df_small_testing2$classe), digits=4)

# Run against 20 testing set.
print(predict(modFit1, newdata=testCleaned))

# Train on training set 3 of 4 with both preprocessing and cross validation using randomForest method.
set.seed(123)
modFit2 <- train(df_small_training3$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training3)
print(modFit2, digits=3)

# Run against testing set 3 of 4.
predictions2 <- predict(modFit2, newdata=df_small_testing3)
print(confusionMatrix(predictions2, df_small_testing3$classe), digits=4)

# Run against 20 testing set.
print(predict(modFit2, newdata=testCleaned))

# Train on training set 4 of 4 with both preprocessing and cross validation using randomForest method.
set.seed(123)
modFit3 <- train(df_small_training4$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training4)
print(modFit3, digits=3)

# Run against testing set 1 of 4.
predictions3 <- predict(modFit3, newdata=df_small_testing4)
print(confusionMatrix(predictions3, df_small_testing4$classe), digits=4)

# Run against 20 testing set.
print(predict(modFit3, newdata=testCleaned))
```
# Out of Sample Error

Out of sample error is the "error rate we get on a new data set.". In the current case, it's the error rate after running the predict() function on the 4 testing sets:
.	Random Forest (preprocessing and cross validation) Testing Set 1: 1 - .9704 = 0.0296
.	Random Forest (preprocessing and cross validation) Testing Set 2: 1 - .9624 = 0.0376
.	Random Forest (preprocessing and cross validation) Testing Set 3: 1 - .9655 = 0.0345
.	Random Forest (preprocessing and cross validation) Testing Set 4: 1 - .9568 = 0.0432
Since each testing set is roughly of equal size, I decided to average the out of sample error rates from test sets 1-4 yielding a predicted out of sample rate of 0.0362.

# CONCLUSION
Two separate predictions were generated by appling the 4 models against the actual 20 item training set:
A) Accuracy Rate 0.0296, 0.0376 and 0.0345 Predictions: B A B A A E D B A A B C B A E E A B B B
B) Accuracy Rate 0.0432 Predictions: B A B A A E D D A A B C B A E E A B B B

The accuracy rate for option A was better than that for option B. Moreover, options A and B above only differed for item 8 (B for option A, D for option B). Therefore I subimitted values from Option A. 


























